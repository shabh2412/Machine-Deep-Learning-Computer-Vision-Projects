# -*- coding: utf-8 -*-
"""Predicting Engagement - What drives ad performance

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GI9MZhJguY5cKn6ppx3gl5ZkEnudzDeI

# Predicting Engagement - What drives ad performance?
"""

import matplotlib.pyplot as plt
import pandas as pd

bank = pd.read_csv('/content/bank-full.csv', sep=';')

bank.head()

# Let's see a summary of our dataframe
print ("Rows     : " , bank.shape[0])
print ("Columns  : " , bank.shape[1])
print ("\nFeatures : \n" ,bank.columns.tolist())
print ("\nMissing values :  ", bank.isnull().sum().values.sum())
print ("\nUnique values :  \n", bank.nunique())

bank.describe()

bank.info()

# Here we use the apply funtion to transform 'y' from yes or no to 0s and 1s
bank['converted'] = bank['y'].apply(lambda x: 0 if x == 'no' else 1)
del bank['y']
bank.head()

# Let's Visualize how our output variable (converted) changes with different incomes
ax = bank[['converted', 'balance']].boxplot(by='converted', showfliers=False, figsize=(10, 7))

ax.set_xlabel('Conversion')
ax.set_ylabel('Average Bank Balance')
ax.set_title('Average Bank Balance Distributions by Conversion')

plt.suptitle("")
plt.show()

# Let's Visualize how our output variable (converted) changes with different incomes
ax = bank[['converted', 'balance']].boxplot(by='converted', showfliers=True, figsize=(10, 7))

ax.set_xlabel('Conversion')
ax.set_ylabel('Average Bank Balance')
ax.set_title('Average Bank Balance Distributions by Conversion')

plt.suptitle("")
plt.show()

# Let's do the same withing using Violin plots
import seaborn as sns

fontsize = 10

fig, axes = plt.subplots()
# plot violin. 'Scenario' is according to x axis, 
# 'LMP' is y axis, data is your dataframe. ax - is axes instance


fig.set_size_inches(10, 8)

sns.violinplot('converted','balance', data=bank, ax = axes)
axes.set_title('Average Bank Balance Distributions by Conversion')

axes.yaxis.grid(True)
axes.set_xlabel('Conversion')
axes.set_ylabel('Average Bank Balance')

plt.show()

bank['campaign'].unique()

# Conversion rate by campaign
conversions_by_contacts = bank.groupby('campaign')['converted'].sum() / bank.groupby('campaign')['converted'].count() * 100.0
# Let's see the top ten campaigns in terms of % converted
conversions_by_contacts.head(10)

ax = conversions_by_contacts.plot(
    kind='bar',
    figsize=(10, 8),
    title='Conversion Rates by Number of Contacts',
    grid=True,
    color='coral'
)

ax.set_xlabel('Number of Contacts')
ax.set_ylabel('Conversion Rate (%)')

plt.show()

# How about conversion rate by job?
conversion_rate_by_job = bank.groupby(by='job')['converted'].sum() / bank.groupby(by='job')['converted'].count() * 100.0
ax = conversion_rate_by_job.plot(kind='barh', grid=True, title='Conversion Rates by Job')

ax.set_xlabel('Conversion Rate (%)')
ax.set_ylabel('Job')

plt.show()

# View the number of unique elements in each feature
bank.nunique()

# Get our category type columns
cols = bank.columns
num_cols = bank._get_numeric_data().columns
cat_cols = list(set(cols) - set(num_cols))
cat_cols

"""# We need to encode our cateogorical varaibles 
marital',
 'loan',
 'month',
 'contact',
 'housing',
 'education',
 'poutcome',
 'default',
 'job'
"""

# Starting with month first
bank['month'].unique()

bank.groupby('month').count()['converted']

months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']

bank['month'] = bank['month'].apply(lambda x: months.index(x)+1)
bank.head()

"""### Let's encode jobs & marital"""

bank['job'].unique()

bank = pd.get_dummies(data=bank, columns=['job'])
bank.head()

bank = pd.get_dummies(data=bank, columns=['marital'])
bank.head()

"""### Encoding Housing """

bank['housing'].unique()

bank['housing'] = bank['housing'].map(lambda s :1  if s == 'yes' else 0)
bank.head()

"""### Encoding loans """

bank['loan'].unique()

bank['loan'] = bank['loan'].map(lambda s :1  if s == 'yes' else 0)
bank.head()

bank.columns

bank.head()

bank['education'].unique()

bank = pd.get_dummies(data=bank, columns=['education'])
bank.head()

#default, , contact
bank['contact'].unique()

bank = pd.get_dummies(data=bank, columns=['contact'])
bank.head()

bank['default'].unique()

bank = pd.get_dummies(data=bank, columns=['default'])
bank.head()

bank['poutcome'].unique()

bank = pd.get_dummies(data=bank, columns=['poutcome'])
bank.head()

# What categoric columns are left?
cols = bank.columns
num_cols = bank._get_numeric_data().columns
cat_cols = list(set(cols) - set(num_cols))
cat_cols

Y_train = bank['converted']
X_train = bank.drop(labels = ["converted"], axis = 1)
X_train

"""# Now let's Fit Our Decision Tree Model"""

from sklearn import tree

dec_tree_model = tree.DecisionTreeClassifier(max_depth=5)

dec_tree_model.fit(X_train, Y_train)

# Install graphviz if you need to install the module
#!pip install graphviz

features = list(X_train.columns)
response_var = 'converted'

features

"""# Generate and Visualize Our Decision Tree"""

import graphviz

# We export our tree to a DOT format is a graphic description language
dot_data = tree.export_graphviz(dec_tree_model, feature_names=features, class_names=['0', '1'],
                                filled=True, rounded=True, special_characters=True) 

# Create a visual graph of our tree
graph = graphviz.Source(dot_data)

"""# Understanding our Tree
- The first line contains split threshold 
- The second line is the Gini impurity which is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset
- The third line gives us the total number of records that belong to that node
- The fourth line in each node gives us the composition of the records in two different classes.
- The fifth line is the class prediction (only use as a predictor when looking at the bottom nodes or root nodes)
"""

# Display our tree below
from IPython.core.display import display, HTML
display(HTML("<style>text {font-size: 10px;}</style>"))

graph

from sklearn.base import clone 

def imp_df(column_names, importances):
  df = pd.DataFrame({'feature': column_names, 
                     'feature_importance': importances}).sort_values('feature_importance', ascending = False).reset_index(drop = True)
  return df

def drop_col_feat_imp(model, X_train, y_train, random_state = 42):
    
    # clone the model to have the exact same specification as the one initially trained
    model_clone = clone(model)
    # set random_state for comparability
    model_clone.random_state = random_state
    # training and scoring the benchmark model
    model_clone.fit(X_train, y_train)
    benchmark_score = model_clone.score(X_train, y_train)
    # list for storing feature importances
    importances = []
    
    # iterating over all columns and storing feature importance (difference between benchmark and new model)
    for col in X_train.columns:
        model_clone = clone(model)
        model_clone.random_state = random_state
        model_clone.fit(X_train.drop(col, axis = 1), y_train)
        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)
        importances.append(benchmark_score - drop_col_score)
    
    importances_df = imp_df(X_train.columns, importances)
    return importances_df

drop_col_feat_imp(dec_tree_model, X_train, Y_train)